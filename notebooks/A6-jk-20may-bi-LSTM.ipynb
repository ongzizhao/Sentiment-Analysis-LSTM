{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d73eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Number of GPUs Available: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16109232823816068962,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2959205990\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13035064519604526319\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check and activate Nvidia GPU\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "\n",
    "display(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "display(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634d077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import all the reqired modules\n",
    "\n",
    "\"\"\"\n",
    "# %load_ext autoreload\n",
    "# from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.layers import Flatten, GRU, RNN, SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import losses\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8cc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dir of your CSV as well as the X, y columns for prediction\n",
    "\n",
    "data_path = 'c:/users/ezalgo/all-assignments/assignment6/data/data-p2v2.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "y = df['label']\n",
    "X = df['nltk_stem']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34ab30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the set using sklearn's train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb9c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160224 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_features=50000\n",
    "\n",
    "docs = X_train\n",
    "#display(len(docs))\n",
    "t = Tokenizer(num_words=50000)\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "max_length = 350\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "word_index = t.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e30aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "50001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50001, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure the stanford glove directory and the model used\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003663ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 350, 100)          5000100   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 350, 128)          84480     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 350, 128)          512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 350, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 5,184,549\n",
      "Trainable params: 183,937\n",
      "Non-trainable params: 5,000,612\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "embedding glove - lstm\n",
    "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features+1, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "# model.add(Dropout(DROPOUT_RATE))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(DROPOUT_RATE))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d833661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# require this else the model.fit hits an error\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f65c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-8cc14d781c07>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-8cc14d781c07>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    validation_split=0.25,\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "earlystop_cb = [EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-2,\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, \n",
    "          y_train,         \n",
    "          #validation_data = (X_test, y_test),\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          callbacks=earlystop_cb,\n",
    "          validation_split=0.25,\n",
    "          verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a8f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d391d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168aa5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
